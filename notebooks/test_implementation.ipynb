{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Testing ViT implementation\n",
    "This notebook goes through testing my implementation of ViT described in [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929), you can also find the paper under `refrences`.\n",
    "\n",
    "I explain some concepts and add notes along the way."
   ],
   "id": "4bdd040cbed984e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title Import packages\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "import torch\n",
    "import io\n",
    "import urllib.request\n",
    "\n",
    "from data.load_data import load_data\n",
    "from data.utils import patchify, unpatchify\n",
    "from architectures.ClassificationViT import ClassificationViT"
   ],
   "id": "bc517d4dd9a7bc05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@title Download Testing Data\n",
    "def load_from_url(url):\n",
    "    return torch.load(io.BytesIO(urllib.request.urlopen(url).read()))\n",
    "\n",
    "test_data = load_from_url('https://github.com/Berkeley-CS182/cs182hw9/raw/main/test_reference.pt')\n",
    "auto_grader_data = load_from_url('https://github.com/Berkeley-CS182/cs182hw9/raw/main/autograder_student.pt')\n",
    "auto_grader_data['output'] = {}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title Utilities for Testing\n",
    "def save_auto_grader_data():\n",
    "    torch.save(\n",
    "        {'output': auto_grader_data['output']},\n",
    "        'autograder.pt'\n",
    "    )\n",
    "\n",
    "def rel_error(x, y):\n",
    "    return torch.max(\n",
    "        torch.abs(x - y)\n",
    "        / (torch.maximum(torch.tensor(1e-8), torch.abs(x) + torch.abs(y)))\n",
    "    ).item()\n",
    "\n",
    "def check_error(name, x, y, tol=1e-3):\n",
    "    error = rel_error(x, y)\n",
    "    if error > tol:\n",
    "        print(f'The relative error for {name} is {error}, should be smaller than {tol}')\n",
    "    else:\n",
    "        print(f'The relative error for {name} is {error}')\n",
    "\n",
    "def check_acc(acc, threshold):\n",
    "    if acc < threshold:\n",
    "        print(f'The accuracy {acc} should >= threshold accuracy {threshold}')\n",
    "    else:\n",
    "        print(f'The accuracy {acc} is better than threshold accuracy {threshold}')"
   ],
   "id": "5a8d372723e60cbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vision Transformer\n",
    "The first part of this notebook is implementing Vision Transformer (ViT) and training it on CIFAR dataset.\n"
   ],
   "id": "12c6fe734725b9d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Image patchify and unpatchify\n",
    "\n",
    "In ViT, an image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. The architecture can be seen in the following figure.\n",
    "![vit](https://github.com/google-research/vision_transformer/blob/main/vit_figure.png?raw=true)\n",
    "\n",
    "To get started with implementing ViT, we need to implement splitting image batch into fixed-size patches batch in ```patchify``` and combining patches batch into the original image batch in ```unpatchify```. The `patchify` function has been implemented for you. **Please implement `unpatchify`,** assuming that the output image is squared.\n",
    "\n",
    "This implementation uses [einops](https://github.com/arogozhnikov/einops) for flexible tensor operations, you can check out its [tutorial](https://einops.rocks/1-einops-basics/)."
   ],
   "id": "957f6205976e1723"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title Test your implementation\n",
    "x = test_data['input']['patchify']\n",
    "y = test_data['output']['patchify']\n",
    "check_error('patchify', patchify(x), y)\n",
    "\n",
    "x = auto_grader_data['input']['patchify']\n",
    "auto_grader_data['output']['patchify'] = patchify(x)\n",
    "save_auto_grader_data()\n",
    "\n",
    "\n",
    "x = test_data['input']['unpatchify']\n",
    "y = test_data['output']['unpatchify']\n",
    "check_error('unpatchify', unpatchify(x), y)\n",
    "\n",
    "x = auto_grader_data['input']['unpatchify']\n",
    "auto_grader_data['output']['unpatchify'] = unpatchify(x)\n",
    "\n",
    "save_auto_grader_data()"
   ],
   "id": "94080eb88cf6f641"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ViT Model\n",
    "\n",
    "The implementation if transformer simply wraps `nn.TransformerEncoder` of PyTorch, you can find it under `architectures/Transformer`. \n",
    "\n",
    "We combine the transformer with the patching and classification head in  `architectures/ClassificationViT`. \n"
   ],
   "id": "5378a7438a61ad27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Testing Implementation",
   "id": "13e5e9c5829af3b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = ClassificationViT(10)\n",
    "model.load_state_dict(test_data['weights']['ClassificationViT'])\n",
    "x = test_data['input']['ClassificationViT.forward']\n",
    "y = model.forward(x)\n",
    "check_error('ClassificationViT.forward', y, test_data['output']['ClassificationViT.forward'])\n",
    "\n",
    "model.load_state_dict(auto_grader_data['weights']['ClassificationViT'])\n",
    "x = auto_grader_data['input']['ClassificationViT.forward']\n",
    "y = model.forward(x)\n",
    "auto_grader_data['output']['ClassificationViT.forward'] = y\n",
    "save_auto_grader_data()"
   ],
   "id": "e9f2b8eeb560a4ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Loader and Preprocess\n",
    "\n",
    "We use ```torchvision``` to download and prepare images and labels. ViT usually works on a much larger image dataset, but due to our limited computational resources, we train our ViT on CIFAR-10.\n",
    "\n",
    "You can find the code under `data/load_data`"
   ],
   "id": "6ea508734274bd27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_loader, test_loader = load_data()\n",
    "train_loader, test_loader"
   ],
   "id": "32be299375a6fdcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised Training ViT\n",
    "You can test the full implementation by running the training script `main.py`. You can find a notebook that does so inside `notebooks/training_test`"
   ],
   "id": "d1a48b30af1832e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
